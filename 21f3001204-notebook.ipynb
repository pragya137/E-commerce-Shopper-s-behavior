{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-03T06:29:30.529569Z","iopub.execute_input":"2023-04-03T06:29:30.530512Z","iopub.status.idle":"2023-04-03T06:29:30.564503Z","shell.execute_reply.started":"2023-04-03T06:29:30.530385Z","shell.execute_reply":"2023-04-03T06:29:30.563026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/e-commerce-shoppers-behaviour-understanding/test_data_v2.csv\")\ntrain_data = pd.read_csv(\"/kaggle/input/e-commerce-shoppers-behaviour-understanding/train_data_v2.csv\")\nsample_data = pd.read_csv(\"/kaggle/input/e-commerce-shoppers-behaviour-understanding/sample.csv\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:30:29.980736Z","iopub.execute_input":"2023-04-03T06:30:29.981178Z","iopub.status.idle":"2023-04-03T06:30:30.126647Z","shell.execute_reply.started":"2023-04-03T06:30:29.981144Z","shell.execute_reply":"2023-04-03T06:30:30.124777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Made a new feature from similar types of numerical features by adding them and then dropped them\n\ntrain_data['HomePage_about']=train_data['HomePage']+train_data['HomePage_Duration']\ntrain_data['LandingPage_about']=train_data['LandingPage']+train_data['LandingPage_Duration']\ntrain_data[\"ProductDescriptionPage_about\"]=train_data[\"ProductDescriptionPage\"]+train_data[\"ProductDescriptionPage_Duration\"]\ntrain_data[\"GoogleMetric_about\"]=train_data['GoogleMetric:Bounce Rates'] +train_data['GoogleMetric:Exit Rates']\n\ntrain_data = train_data.drop(['HomePage', 'HomePage_Duration', 'LandingPage', 'LandingPage_Duration',\n       'ProductDescriptionPage', 'ProductDescriptionPage_Duration',\n       'GoogleMetric:Bounce Rates', 'GoogleMetric:Exit Rates'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:30:33.004033Z","iopub.execute_input":"2023-04-03T06:30:33.005203Z","iopub.status.idle":"2023-04-03T06:30:33.045478Z","shell.execute_reply.started":"2023-04-03T06:30:33.005162Z","shell.execute_reply":"2023-04-03T06:30:33.044044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Same done on test data\n\ntest_data['HomePage_about']=test_data['HomePage']+test_data['HomePage_Duration']\ntest_data['LandingPage_about']=test_data['LandingPage']+test_data['LandingPage_Duration']\ntest_data[\"ProductDescriptionPage_about\"]=test_data[\"ProductDescriptionPage\"]+test_data[\"ProductDescriptionPage_Duration\"]\ntest_data[\"GoogleMetric_about\"]=test_data['GoogleMetric:Bounce Rates'] +test_data['GoogleMetric:Exit Rates']\ntest_data = test_data.drop(['HomePage', 'HomePage_Duration', 'LandingPage', 'LandingPage_Duration',\n       'ProductDescriptionPage', 'ProductDescriptionPage_Duration',\n       'GoogleMetric:Bounce Rates', 'GoogleMetric:Exit Rates'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:32:01.287463Z","iopub.execute_input":"2023-04-03T06:32:01.287930Z","iopub.status.idle":"2023-04-03T06:32:01.305921Z","shell.execute_reply.started":"2023-04-03T06:32:01.287893Z","shell.execute_reply":"2023-04-03T06:32:01.304930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filling the missing values of numerical Columns through KNN Imputation\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, PowerTransformer\n\nfrom sklearn.impute import KNNImputer\n\n\nnumeric_cols = train_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n\nknn_imputer = KNNImputer(n_neighbors=10, missing_values=np.nan)\n\ntrain_data[numeric_cols] = knn_imputer.fit_transform(train_data[numeric_cols])\ntest_data[numeric_cols] = knn_imputer.transform(test_data[numeric_cols])\n\n\n       ","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:32:07.145531Z","iopub.execute_input":"2023-04-03T06:32:07.145994Z","iopub.status.idle":"2023-04-03T06:32:11.186627Z","shell.execute_reply.started":"2023-04-03T06:32:07.145961Z","shell.execute_reply":"2023-04-03T06:32:11.185346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values categorical columns through simple imputation using Mode\n\ncat_cols = train_data.select_dtypes(include=['object']).columns.tolist()\nmode_imputer = SimpleImputer(strategy='most_frequent')\ntrain_data[cat_cols] = mode_imputer.fit_transform(train_data[cat_cols])\ntest_data[cat_cols] = mode_imputer.transform(test_data[cat_cols])\n\n\n#Applied One-hot encoding to the categorical columns\n\n\ncols_to_encode = cat_cols\nencoder = OneHotEncoder(handle_unknown='ignore')\nencoded_cols_train = encoder.fit_transform(train_data[cols_to_encode])\nencoded_cols_test = encoder.transform(test_data[cols_to_encode])\n\nencoded_cols_train_df = pd.DataFrame(encoded_cols_train.toarray(), columns=encoder.get_feature_names_out(cols_to_encode))\nencoded_cols_test_df = pd.DataFrame(encoded_cols_test.toarray(), columns=encoder.get_feature_names_out(cols_to_encode))\n\n#Concatenate the numerical and encoded- categorical columns, then removed the categorical columns\n\ntrain_data = pd.concat([train_data, encoded_cols_train_df], axis=1)\ntest_data = pd.concat([test_data, encoded_cols_test_df], axis=1)\n\ntrain_data.drop(cols_to_encode, axis=1, inplace=True)\ntest_data.drop(cols_to_encode, axis=1, inplace=True)\n\n#Used PowerTransformer to scale the data\n\ncolumns_to_scale = numeric_cols\nscaler = PowerTransformer()\n\ntrain_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\ntest_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:32:16.575735Z","iopub.execute_input":"2023-04-03T06:32:16.576224Z","iopub.status.idle":"2023-04-03T06:32:16.890643Z","shell.execute_reply.started":"2023-04-03T06:32:16.576186Z","shell.execute_reply":"2023-04-03T06:32:16.889379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applied PCA to get relevant features i.e features that captures 99% of the variance\n\nfrom sklearn.decomposition import PCA\ncolumns_to_pca = numeric_cols\npca=PCA()\npca.fit(train_data[columns_to_pca])\nn_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.99) + 1\npca = PCA(n_components=n_components)\n\ntrain_data_pca = pd.DataFrame(pca.fit_transform(train_data[columns_to_pca]), columns=['PCA_' + str(i) for i in range(1, n_components+1)])\ntest_data_pca = pd.DataFrame(pca.transform(test_data[columns_to_pca]), columns=['PCA_' + str(i) for i in range(1, n_components+1)])\n\ntrain_data = pd.concat([train_data, train_data_pca], axis=1)\ntest_data = pd.concat([test_data, test_data_pca], axis=1)\n\ntrain_data.drop(columns_to_pca, axis=1, inplace=True)\ntest_data.drop(columns_to_pca, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:32:21.246168Z","iopub.execute_input":"2023-04-03T06:32:21.246579Z","iopub.status.idle":"2023-04-03T06:32:21.319733Z","shell.execute_reply.started":"2023-04-03T06:32:21.246545Z","shell.execute_reply":"2023-04-03T06:32:21.318080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nX_train = train_data.drop('Made_Purchase', axis='columns')\ny_train = train_data['Made_Purchase']\n","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:32:26.449123Z","iopub.execute_input":"2023-04-03T06:32:26.450329Z","iopub.status.idle":"2023-04-03T06:32:26.459449Z","shell.execute_reply.started":"2023-04-03T06:32:26.450269Z","shell.execute_reply":"2023-04-03T06:32:26.457332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Used Voting Classifier with multiple models\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nX_train, y_train = shuffle(X_train, y_train, random_state=0)\n\n\nmodel=VotingClassifier(estimators=[\n    ('lr', LogisticRegression(solver='liblinear',penalty='l2', random_state=0) ),\n    ('kn', KNeighborsClassifier(n_neighbors=50)),\n    ('adb',AdaBoostClassifier())\n    ],\n    voting='soft')\nmodel.fit(X_train, y_train)\nres = cross_val_score(model, X_train, y_train, cv=35)\nres.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:32:29.749587Z","iopub.execute_input":"2023-04-03T06:32:29.750031Z","iopub.status.idle":"2023-04-03T06:33:39.921256Z","shell.execute_reply.started":"2023-04-03T06:32:29.749997Z","shell.execute_reply":"2023-04-03T06:33:39.920047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T05:59:12.831591Z","iopub.execute_input":"2023-04-01T05:59:12.831982Z","iopub.status.idle":"2023-04-01T05:59:12.838542Z","shell.execute_reply.started":"2023-04-01T05:59:12.831950Z","shell.execute_reply":"2023-04-01T05:59:12.836985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train)\ny_pred = model.predict(test_data)\ny_pred = y_pred.astype(bool)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:34:01.462120Z","iopub.execute_input":"2023-04-03T06:34:01.462560Z","iopub.status.idle":"2023-04-03T06:34:05.656582Z","shell.execute_reply.started":"2023-04-03T06:34:01.462525Z","shell.execute_reply":"2023-04-03T06:34:05.655392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsub = pd.DataFrame({'Id': [i for i in range(test_data.shape[0])],'Made_Purchase': y_pred} )\nsub.to_csv(\"testOutput.csv\", index=False)\n\noutput = pd.read_csv(\"testOutput.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-03T06:34:09.219176Z","iopub.execute_input":"2023-04-03T06:34:09.219942Z","iopub.status.idle":"2023-04-03T06:34:09.244529Z","shell.execute_reply.started":"2023-04-03T06:34:09.219892Z","shell.execute_reply":"2023-04-03T06:34:09.243477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}